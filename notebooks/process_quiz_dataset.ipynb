{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Quiz Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook preprocesses the JSON file related to quizzes located at quiz-app/src/assets/translations/es.json in the repository https://github.com/microsoft/ML-For-Beginners. The quizzes provided are in multiple-choice question (MCQ) format across various topics. Our goal is to extract the questions and answers to create an evaluation and test dataset, where each example contains the \"question\" and \"answer\" as JSON fields. \n",
    " \n",
    "We will create two datasets: one with MCQ-type questions that include options, which can be generated by directly extracting the relevant keys for questions and answers from each topic, and another with open-ended questions that do not include options. \n",
    " \n",
    "Since some questions contain options that provide additional context for answering, such as fill-in-the-blank formats or options like \"both of these,\" we need to rephrase these questions to ensure that answering does not require context from the options. While this could be done manually, we will utilize a language model (LLM) and its tool-calling feature to provide examples as context to the LLM, instructing it to convert the questions into open-ended question and answer pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter you Google API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Quiz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../corpus/itml-quizzes.json', 'r') as f:\n",
    "    quiz_obj = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the question and answer pairs from the JSON structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "lesson_quizzes: list = quiz_obj[0][\"quizzes\"]\n",
    "for quiz_info in lesson_quizzes:\n",
    "    title = quiz_info[\"title\"]  # Lesson wise title\n",
    "    quizzes = quiz_info[\"quiz\"]  # List of quiz for a each lesson\n",
    "    for quiz in quizzes:\n",
    "        question_text = quiz[\"questionText\"]\n",
    "        answer = list(\n",
    "            filter(lambda item: item[\"isCorrect\"] == \"true\", quiz[\"answerOptions\"])\n",
    "        )[0][\"answerText\"]\n",
    "\n",
    "        mcq = \"\\n\".join([\"- \" + item[\"answerText\"] for item in quiz[\"answerOptions\"]])\n",
    "        question = f\"Question:\\n{question_text}\\n\\nOptions:\\n{mcq}\"\n",
    "\n",
    "        dataset.append({\"question\": question, \"answer\": answer})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display a Sample Question and Answer Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "What is the technical difference between classical ML and deep learning?\n",
      "\n",
      "Options:\n",
      "- classical ML was invented first\n",
      "- the use of neural networks\n",
      "- deep learning is used in robots\n",
      "\n",
      "Answer:\n",
      "the use of neural networks\n"
     ]
    }
   ],
   "source": [
    "print(dataset[1][\"question\"]) \n",
    "print(f'\\nAnswer:\\n{dataset[1][\"answer\"]}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Open Ended Questions from MCQ formatted questions using LLM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This notebook utilizes Gemini models via the Instructor package to rephrase multiple-choice questions (MCQs). To run the following cells, a `GOOGLE_API_KEY` is required. You can store this key in a .env file located in the root of the project, enter it directly in the input box when prompted, or set it directly in the environment variable using `os.environ[\"GOOGLE_API_KEY\"]`. If you wish to use a different model, simply modify the instructor client instance and provide the necessary API key for that platform in the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter you Google API key: \") \n",
    "\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = <GOOGLE_API_KEY>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "import instructor\n",
    "import google.generativeai as genai\n",
    "from limiter import Limiter\n",
    "load_dotenv()\n",
    "\n",
    "class Example(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "instructor_client = instructor.from_gemini(\n",
    "    client=genai.GenerativeModel(model_name='gemini-2.0-flash'), mode=instructor.Mode.GEMINI_JSON, use_async=False\n",
    ")\n",
    "\n",
    "# These values are specific to the gemini-2.0-flash model free tier rpm. \n",
    "limiter = Limiter(rate=0.15, consume=1, capacity=1)\n",
    "\n",
    "@limiter\n",
    "def generate_open_qa(mcqs):\n",
    "    # Meta prompting link :) https://chatgpt.com/share/67e40d4e-a764-8006-a436-768679e5fdcd\n",
    "    prompt = f\"\"\"Convert the following multiple-choice question (MCQ) and Answer pairs into an open-ended question and answer pairs. Ensure that:\n",
    "    1. The rephrased question does not rely on the provided answer choices.\n",
    "    2. The modified answer remains accurate and meaningful and align with the original answer.\n",
    "    3. If the question includes options like \"Both of the above\" or \"True/False,\" adjust the answer to be self-contained.\n",
    "    4. If the question is a fill-in-the-blank type, rewrite it as a complete question.\n",
    "\n",
    "    MCQS:\n",
    "    {\"\\n\".join([str(mcq) for mcq in mcqs])}\n",
    "    \"\"\"\n",
    "    return instructor_client.messages.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        response_model=list[Example],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate 10 examples per request, as larger contexts may negatively impact output quality. \n",
    "\n",
    "> Note: This number is not derived from any specific experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335c289a181043f7aad2617202fce57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating QA pairs::   0%|          | 0/156 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from time import sleep\n",
    "\n",
    "bar = tqdm(total=len(dataset), desc=\"Generating QA pairs:\")\n",
    "qa_dataset = []\n",
    "for i in range(0, len(dataset), 10):\n",
    "    sleep(0.5)\n",
    "    start, end = i, min(i + 10, len(dataset))\n",
    "    qa_dataset += generate_open_qa(dataset[start:end])\n",
    "    bar.update(min(10, len(dataset) - i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are machine learning applications prevalent in...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the primary technical distinction betw...</td>\n",
       "      <td>Deep learning utilizes neural networks, while ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are some reasons a business might impleme...</td>\n",
       "      <td>Businesses might use ML strategies to automate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the primary concept that machine learn...</td>\n",
       "      <td>Machine learning algorithms are designed to si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Could you provide an example of a classical ma...</td>\n",
       "      <td>An example of a classical machine learning tec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Are machine learning applications prevalent in...   \n",
       "1  What is the primary technical distinction betw...   \n",
       "2  What are some reasons a business might impleme...   \n",
       "3  What is the primary concept that machine learn...   \n",
       "4  Could you provide an example of a classical ma...   \n",
       "\n",
       "                                              answer  \n",
       "0                                               True  \n",
       "1  Deep learning utilizes neural networks, while ...  \n",
       "2  Businesses might use ML strategies to automate...  \n",
       "3  Machine learning algorithms are designed to si...  \n",
       "4  An example of a classical machine learning tec...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_qa_dataset = pd.DataFrame([dict(qa) for qa in qa_dataset]) # casting to dict is required since the examples are instances of the Example object\n",
    "df_qa_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Question:\\nApplications of machine learning ar...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Question:\\nWhat is the technical difference be...</td>\n",
       "      <td>the use of neural networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Question:\\nWhy might a business want to use ML...</td>\n",
       "      <td>both of the above</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Question:\\nMachine learning algorithms are mea...</td>\n",
       "      <td>the human brain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Question:\\nWhat is an example of a classical M...</td>\n",
       "      <td>natural language processing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Question:\\nApplications of machine learning ar...   \n",
       "1  Question:\\nWhat is the technical difference be...   \n",
       "2  Question:\\nWhy might a business want to use ML...   \n",
       "3  Question:\\nMachine learning algorithms are mea...   \n",
       "4  Question:\\nWhat is an example of a classical M...   \n",
       "\n",
       "                        answer  \n",
       "0                         True  \n",
       "1   the use of neural networks  \n",
       "2            both of the above  \n",
       "3              the human brain  \n",
       "4  natural language processing  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset = pd.DataFrame(dataset)\n",
    "df_dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the data and create a train(evaluation set) & test split of both the datsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "# Setting frac=1 effectively shuffles the dataset\n",
    "shuffled_indices = df_dataset.sample(frac=1, random_state=random_seed).index \n",
    "\n",
    "train_indices = shuffled_indices[:100]\n",
    "test_indices = shuffled_indices[100:]\n",
    "\n",
    "train_df = df_dataset.loc[train_indices]\n",
    "test_df = df_dataset.loc[test_indices]\n",
    "\n",
    "train_qa_df = df_qa_dataset.loc[train_indices]\n",
    "test_qa_df = df_qa_dataset.loc[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display a Sample QA pair before and after QA Repharsing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "The process of splitting a dataset into a certain ratio of training and testing dataset using Scikit Learn's 'train_test_split()' method/function is called:\n",
      "\n",
      "Options:\n",
      "- Cross-Validation\n",
      "- Hold-Out Validation\n",
      "- Leave one out Validation\n",
      "\n",
      "Answer:\n",
      " Hold-Out Validation\n"
     ]
    }
   ],
   "source": [
    "print(train_df.iloc[5]['question'])\n",
    "print(\"\\nAnswer:\\n\", train_df.iloc[5]['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      " What is the name of the process of splitting a dataset into training and testing sets using Scikit Learn's 'train_test_split()' method/function?\n",
      "\n",
      "Answer:\n",
      " This process is called Hold-Out Validation.\n"
     ]
    }
   ],
   "source": [
    "print(\"Question:\\n\",train_qa_df.iloc[5]['question'])\n",
    "print(\"\\nAnswer:\\n\", train_qa_df.iloc[5]['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the splits as JSONL files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_json('../datasets/itml/itml_mcq_eval.jsonl', lines=True, orient='records')\n",
    "test_df.to_json('../datasets/itml/itml_mcq_test.jsonl', lines=True, orient='records')\n",
    "train_qa_df.to_json('../datasets/itml/itml_qa_eval.jsonl', lines=True, orient='records')\n",
    "test_qa_df.to_json('../datasets/itml/itml_qa_test.jsonl', lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample and save various fractions of the training a.k.a evaluation set for preliminary assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = [10, 30, 50]\n",
    "for n in sample_sizes:\n",
    "    sample_df = train_df.sample(n) \n",
    "    sample_qa_df = train_qa_df.loc[sample_df.index]\n",
    "    sample_df.to_json(f'../datasets/itml/itml_mcq_eval_{n}_samples.jsonl', lines=True, orient='records')\n",
    "    sample_qa_df.to_json(f'../datasets/itml/itml_qa_eval_{n}_samples.jsonl', lines=True, orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ask-notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
