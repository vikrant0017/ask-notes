{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deepeval in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (2.5.2)\n",
      "Requirement already satisfied: requests in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (4.67.1)\n",
      "Requirement already satisfied: pytest in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (8.3.5)\n",
      "Requirement already satisfied: tabulate in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (0.9.0)\n",
      "Requirement already satisfied: typer in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (0.15.1)\n",
      "Requirement already satisfied: rich in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (13.9.4)\n",
      "Requirement already satisfied: protobuf in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (5.29.3)\n",
      "Requirement already satisfied: pydantic in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (2.10.6)\n",
      "Requirement already satisfied: sentry-sdk in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (2.21.0)\n",
      "Requirement already satisfied: pytest-repeat in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (0.9.3)\n",
      "Requirement already satisfied: pytest-xdist in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (3.6.1)\n",
      "Requirement already satisfied: portalocker in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (3.1.1)\n",
      "Requirement already satisfied: langchain in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (0.3.15)\n",
      "Requirement already satisfied: llama-index in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (0.12.23)\n",
      "Requirement already satisfied: langchain-core in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (0.3.34)\n",
      "Requirement already satisfied: langchain_openai in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (0.3.5)\n",
      "Requirement already satisfied: langchain-community in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (0.3.15)\n",
      "Requirement already satisfied: docx2txt~=0.8 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (0.8)\n",
      "Requirement already satisfied: importlib-metadata>=6.0.2 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (7.2.1)\n",
      "Requirement already satisfied: tenacity<=9.0.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (8.5.0)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.24.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.24.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (1.29.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.67.1 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (1.70.0)\n",
      "Requirement already satisfied: nest-asyncio in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (1.6.0)\n",
      "Requirement already satisfied: datasets in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (3.2.0)\n",
      "Requirement already satisfied: ollama in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deepeval) (0.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from importlib-metadata>=6.0.2->deepeval) (3.21.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from opentelemetry-api<2.0.0,>=1.24.0->deepeval) (1.2.15)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.66.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.29.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.29.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from opentelemetry-sdk<2.0.0,>=1.24.0->deepeval) (0.50b0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from opentelemetry-sdk<2.0.0,>=1.24.0->deepeval) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from datasets->deepeval) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from datasets->deepeval) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from datasets->deepeval) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from datasets->deepeval) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from datasets->deepeval) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from datasets->deepeval) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from datasets->deepeval) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->deepeval) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from datasets->deepeval) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from datasets->deepeval) (0.27.1)\n",
      "Requirement already satisfied: packaging in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from datasets->deepeval) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from datasets->deepeval) (6.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from requests->deepeval) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from requests->deepeval) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from requests->deepeval) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from requests->deepeval) (2024.12.14)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from langchain->deepeval) (2.0.37)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from langchain->deepeval) (0.3.5)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from langchain->deepeval) (0.3.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from langchain-core->deepeval) (1.33)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from pydantic->deepeval) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from pydantic->deepeval) (2.27.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from langchain-community->deepeval) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from langchain-community->deepeval) (0.4.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from langchain-community->deepeval) (2.7.1)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from langchain_openai->deepeval) (1.61.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from langchain_openai->deepeval) (0.8.0)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from llama-index->deepeval) (0.4.6)\n",
      "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.1 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from llama-index->deepeval) (0.4.1)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.23 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from llama-index->deepeval) (0.12.23.post2)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from llama-index->deepeval) (0.3.1)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from llama-index->deepeval) (0.6.8)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from llama-index->deepeval) (0.3.25)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from llama-index->deepeval) (0.4.3)\n",
      "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from llama-index->deepeval) (0.3.1)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from llama-index->deepeval) (0.3.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from llama-index->deepeval) (0.4.6)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from llama-index->deepeval) (0.4.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from llama-index->deepeval) (3.9.1)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from ollama->deepeval) (0.28.1)\n",
      "Requirement already satisfied: iniconfig in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from pytest->deepeval) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from pytest->deepeval) (1.5.0)\n",
      "Requirement already satisfied: execnet>=2.1 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from pytest-xdist->deepeval) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from rich->deepeval) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from rich->deepeval) (2.19.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from typer->deepeval) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from typer->deepeval) (1.5.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from aiohttp->datasets->deepeval) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from aiohttp->datasets->deepeval) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from aiohttp->datasets->deepeval) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from aiohttp->datasets->deepeval) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from aiohttp->datasets->deepeval) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from aiohttp->datasets->deepeval) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from aiohttp->datasets->deepeval) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->deepeval) (3.26.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->deepeval) (0.9.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from deprecated>=1.2.6->opentelemetry-api<2.0.0,>=1.24.0->deepeval) (1.17.2)\n",
      "Requirement already satisfied: anyio in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from httpx<0.29,>=0.27->ollama->deepeval) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from httpx<0.29,>=0.27->ollama->deepeval) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama->deepeval) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core->deepeval) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.17->langchain->deepeval) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.17->langchain->deepeval) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.17->langchain->deepeval) (0.23.0)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from llama-index-core<0.13.0,>=0.12.23->llama-index->deepeval) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from llama-index-core<0.13.0,>=0.12.23->llama-index->deepeval) (1.2.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from llama-index-core<0.13.0,>=0.12.23->llama-index->deepeval) (3.4.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from llama-index-core<0.13.0,>=0.12.23->llama-index->deepeval) (10.4.0)\n",
      "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.13 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->deepeval) (0.1.14)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->deepeval) (4.13.3)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->deepeval) (5.3.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->deepeval) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index->deepeval) (0.6.4.post1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich->deepeval) (0.1.2)\n",
      "Requirement already satisfied: joblib in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from nltk>3.8.1->llama-index->deepeval) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from nltk>3.8.1->llama-index->deepeval) (2024.11.6)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from openai<2.0.0,>=1.58.1->langchain_openai->deepeval) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from openai<2.0.0,>=1.58.1->langchain_openai->deepeval) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from openai<2.0.0,>=1.58.1->langchain_openai->deepeval) (1.3.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->deepeval) (1.0.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from SQLAlchemy<3,>=1.4->langchain->deepeval) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from pandas->datasets->deepeval) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from pandas->datasets->deepeval) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from pandas->datasets->deepeval) (2024.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index->deepeval) (2.6)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.4 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->deepeval) (0.6.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets->deepeval) (1.17.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/vikrant/miniconda3/envs/ask-notes/lib/python3.13/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->deepeval) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U deepeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.synthesizer import Synthesizer\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter you Google API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "\n",
    "LLM_MODEL = \"gemini-2.0-flash\"\n",
    "EMBEDDING_MODEL = \"models/text-embedding-004\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiChat(DeepEvalBaseLLM):\n",
    "    def __init__(self, model_name: str):\n",
    "        super().__init__(model_name)  # sets model_name and loads model\n",
    "\n",
    "    def load_model(self):\n",
    "        free_tier_rpm_mapping = {\n",
    "            \"gemini-2.0-flash\": 15,\n",
    "            \"gemini-2.0-flash-lite\": 30,\n",
    "        }\n",
    "\n",
    "        # compute request per second\n",
    "        rps = free_tier_rpm_mapping[LLM_MODEL] / 60  # simple min to sec conversion\n",
    "\n",
    "        rate_limiter = InMemoryRateLimiter(\n",
    "            requests_per_second=rps,\n",
    "            check_every_n_seconds=0.1,\n",
    "            max_bucket_size=1,\n",
    "        )\n",
    "        return ChatGoogleGenerativeAI(model=self.model_name, rate_limiter=rate_limiter)\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        res = self.model.invoke(prompt)\n",
    "        return res.content\n",
    "\n",
    "    async def a_generate(self, prompt) -> str:\n",
    "        res = await self.model.ainvoke(prompt)\n",
    "        return res.content\n",
    "\n",
    "    def get_model_name(self) -> str:\n",
    "        return self.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_123504/1963316488.py:14: LangChainBetaWarning: Introduced in 0.2.24. API subject to change.\n",
      "  rate_limiter = InMemoryRateLimiter(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of India is **New Delhi**.\n"
     ]
    }
   ],
   "source": [
    "gen = GeminiChat(model_name=LLM_MODEL)\n",
    "print(gen.generate(\"What is the capital of India?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from deepeval.models import DeepEvalBaseEmbeddingModel\n",
    "\n",
    "class GeminiEmbedding(DeepEvalBaseEmbeddingModel):\n",
    "    model: GoogleGenerativeAIEmbeddings\n",
    "\n",
    "    def __init__(self, model_name: str):\n",
    "        super().__init__(model_name) \n",
    "\n",
    "    def load_model(self):\n",
    "        return GoogleGenerativeAIEmbeddings(model=self.model_name)\n",
    "\n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        return self.model.embed_query(text)\n",
    "\n",
    "    async def a_embed_text(self, text: str) -> List[float]:\n",
    "        return await self.model.aembed_query(text)\n",
    "\n",
    "    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self.model.embed_documents(texts)\n",
    "\n",
    "    async def a_embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        return await self.model.aembed_documents(texts)\n",
    "\n",
    "    def get_model_name(self) -> str:\n",
    "        return self.model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = GeminiEmbedding(EMBEDDING_MODEL)\n",
    "embedding = emb.embed_text(\"What is the capital of India?\")\n",
    "len(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the persistence embeddings in vectordb from previous runs. This can cause issues\n",
    "%rm -rf .vector_db/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "✨ 🚀 ✨ Loading Documents: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n",
      "✨ 📚 ✨ Chunking Documents: 100%|██████████| 1/1 [00:02<00:00,  2.47s/it]\n",
      "✨ 🧩 ✨ Generating Contexts: 100%|██████████| 15/15 [00:15<00:00,  1.05s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Utilizing <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> out of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span> chunks.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Utilizing \u001b[1;36m8\u001b[0m out of \u001b[1;36m21\u001b[0m chunks.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "✨ Generating up to 10 goldens using DeepEval (using gemini-2.0-flash and models/text-embedding-004, method=docs): 100%|██████████| 10/10 [03:16<00:00, 19.62s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Golden(input=\"What qualities correlate strongly to success in DL, according to Karpathy's blog?\", actual_output=None, expected_output=\"According to Andrej Karpathy's blog, the qualities that correlate most strongly to success in deep learning are patience and attention to detail.\", context=['Andrej Karpathy blog\\nA Recipe for Training Neural Networks\\n\\nAbout\\nAuthor: Andrej Karpathy\\nDate: Apr 25, 2019\\n\\nSome few weeks ago I posted a tweet on “the most common neural net mistakes”, listing a few common gotchas related to training neural nets. The tweet got quite a bit more engagement than I anticipated (including a webinar :)). Clearly, a lot of people have personally encountered the large gap between “here is how a convolutional layer works” and “our convnet achieves state of the art results”.\\n\\nSo I thought it could be fun to brush off my dusty blog to expand my tweet to the long form that this topic deserves. However, instead of going into an enumeration of more common errors or fleshing them out, I wanted to dig a bit deeper and talk about how one can avoid making these errors altogether (or fix them very fast). The trick to doing so is to follow a certain process, which as far as I can tell is not very often documented. Let’s start with two important observations that motivate it.\\n\\n1) Neural net training is a leaky abstraction\\n\\n', '.\\n\\nAs a result, (and this is reeaally difficult to over-emphasize) a “fast and furious” approach to training neural networks does not work and only leads to suffering. Now, suffering is a perfectly natural part of getting a neural network to work well, but it can be mitigated by being thorough, defensive, paranoid, and obsessed with visualizations of basically every possible thing. The qualities that in my experience correlate most strongly to success in deep learning are patience and attention to detail.\\n\\nThe recipe\\n\\nIn light of the above two facts, I have developed a specific process for myself that I follow when applying a neural net to a new problem, which I will try to describe. You will see that it takes the two principles above very seriously. In particular, it builds from simple to complex and at every step of the way we make concrete hypotheses about what will happen and then either validate them with an experiment or investigate until we find some issue. What we try to prevent very hard is the introduction of a lot of “unverified” complexity at once, which is bound to introduce bugs/misconfigurations that will take forever to find (if ever). If writing your neural'], retrieval_context=None, additional_metadata={'evolutions': ['Constrained'], 'synthetic_input_quality': 1.0, 'context_quality': 0.8250000000000001}, comments=None, tools_called=None, expected_tools=None, source_file='Neural Network Training Recipe.txt'),\n",
       " Golden(input='How do patience & attention to detail relate to achieving success in DL, acc. to Karpathy?', actual_output=None, expected_output='According to Andrej Karpathy, the qualities that correlate most strongly to success in deep learning are patience and attention to detail.', context=['Andrej Karpathy blog\\nA Recipe for Training Neural Networks\\n\\nAbout\\nAuthor: Andrej Karpathy\\nDate: Apr 25, 2019\\n\\nSome few weeks ago I posted a tweet on “the most common neural net mistakes”, listing a few common gotchas related to training neural nets. The tweet got quite a bit more engagement than I anticipated (including a webinar :)). Clearly, a lot of people have personally encountered the large gap between “here is how a convolutional layer works” and “our convnet achieves state of the art results”.\\n\\nSo I thought it could be fun to brush off my dusty blog to expand my tweet to the long form that this topic deserves. However, instead of going into an enumeration of more common errors or fleshing them out, I wanted to dig a bit deeper and talk about how one can avoid making these errors altogether (or fix them very fast). The trick to doing so is to follow a certain process, which as far as I can tell is not very often documented. Let’s start with two important observations that motivate it.\\n\\n1) Neural net training is a leaky abstraction\\n\\n', '.\\n\\nAs a result, (and this is reeaally difficult to over-emphasize) a “fast and furious” approach to training neural networks does not work and only leads to suffering. Now, suffering is a perfectly natural part of getting a neural network to work well, but it can be mitigated by being thorough, defensive, paranoid, and obsessed with visualizations of basically every possible thing. The qualities that in my experience correlate most strongly to success in deep learning are patience and attention to detail.\\n\\nThe recipe\\n\\nIn light of the above two facts, I have developed a specific process for myself that I follow when applying a neural net to a new problem, which I will try to describe. You will see that it takes the two principles above very seriously. In particular, it builds from simple to complex and at every step of the way we make concrete hypotheses about what will happen and then either validate them with an experiment or investigate until we find some issue. What we try to prevent very hard is the introduction of a lot of “unverified” complexity at once, which is bound to introduce bugs/misconfigurations that will take forever to find (if ever). If writing your neural'], retrieval_context=None, additional_metadata={'evolutions': ['Reasoning'], 'synthetic_input_quality': 1.0, 'context_quality': 0.8250000000000001}, comments=None, tools_called=None, expected_tools=None, source_file='Neural Network Training Recipe.txt'),\n",
       " Golden(input='If a novice were to use those libraries, would they truly \"conquer world\"? Why/why not?', actual_output=None, expected_output='No, because neural nets are not \"off-the-shelf\" software like the Requests library example provided. The 30-line miracle snippets give the false impression that this stuff is plug and play.', context=[\"It is allegedly easy to get started with training neural nets. Numerous libraries and frameworks take pride in displaying 30-line miracle snippets that solve your data problems, giving the (false) impression that this stuff is plug and play. It’s common see things like:\\n\\n>>> your_data = # plug your awesome dataset here\\n>>> model = SuperCrossValidator(SuperDuper.fit, your_data, ResNet50, SGDOptimizer)\\n# conquer world here\\n\\n\\nThese libraries and examples activate the part of our brain that is familiar with standard software - a place where clean APIs and abstractions are often attainable. Requests library to demonstrate:\\n\\n>>> r = requests.get('https://api.github.com/user', auth=('user', 'pass'))\\n>>> r.status_code\\n200\\n\\n\\nThat’s cool! A courageous developer has taken the burden of understanding query strings, urls, GET/POST requests, HTTP connections, and so on from you and largely hidden the complexity behind a few lines of code. This is what we are familiar with and expect. Unfortunately, neural nets are nothing like that. They are not “off-\"], retrieval_context=None, additional_metadata={'evolutions': ['Hypothetical'], 'synthetic_input_quality': 1.0, 'context_quality': 0.775}, comments=None, tools_called=None, expected_tools=None, source_file='Neural Network Training Recipe.txt'),\n",
       " Golden(input='Compare the impression given by neural net libraries to the reality of using neural nets.', actual_output=None, expected_output='Neural net libraries give the false impression that using neural nets is plug and play. In reality, neural nets are not like standard software with clean APIs and attainable abstractions.', context=[\"It is allegedly easy to get started with training neural nets. Numerous libraries and frameworks take pride in displaying 30-line miracle snippets that solve your data problems, giving the (false) impression that this stuff is plug and play. It’s common see things like:\\n\\n>>> your_data = # plug your awesome dataset here\\n>>> model = SuperCrossValidator(SuperDuper.fit, your_data, ResNet50, SGDOptimizer)\\n# conquer world here\\n\\n\\nThese libraries and examples activate the part of our brain that is familiar with standard software - a place where clean APIs and abstractions are often attainable. Requests library to demonstrate:\\n\\n>>> r = requests.get('https://api.github.com/user', auth=('user', 'pass'))\\n>>> r.status_code\\n200\\n\\n\\nThat’s cool! A courageous developer has taken the burden of understanding query strings, urls, GET/POST requests, HTTP connections, and so on from you and largely hidden the complexity behind a few lines of code. This is what we are familiar with and expect. Unfortunately, neural nets are nothing like that. They are not “off-\"], retrieval_context=None, additional_metadata={'evolutions': ['Comparative'], 'synthetic_input_quality': 0.7, 'context_quality': 0.775}, comments=None, tools_called=None, expected_tools=None, source_file='Neural Network Training Recipe.txt'),\n",
       " Golden(input='Why is silently failing during neural net training a common issue, and what factors contribute to this?', actual_output=None, expected_output='Neural net training often fails silently because the possible error surface is large, logical, and tricky to unit test. Factors contributing to this include forgetting to flip labels during data augmentation, autoregressive models accidentally taking the thing it’s trying to predict as an input, clipping the loss instead of the gradients, not using the original mean when initializing weights from a pretrained checkpoint, or messing up the settings for regularization strengths, learning rate, its decay rate, model size, etc.', context=['the-shelf” technology the second you deviate slightly from training an ImageNet classifier. I’ve tried to make this point in my post “Yes you should understand backprop” by picking on backpropagation and calling it a “leaky abstraction”, but the situation is unfortunately much more dire. Backprop + SGD does not magically make your network work. Batch norm does not magically make it converge faster. RNNs don’t magically let you “plug in” text. And just because you can formulate your problem as RL doesn’t mean you should. If you insist on using the technology without understanding how it works you are likely to fail. Which brings me to…\\n\\n2) Neural net training fails silently\\n\\nWhen you break or misconfigure code you will often get some kind of an exception. You plugged in an integer where something expected a string. The function only expected 3 arguments. This import failed. That key does not exist. The number of elements in the two lists isn’t equal. In addition, it’s often possible to create unit tests for a certain functionality.\\n\\nThis is just a start', ' when it comes to training neural nets. Everything could be correct syntactically, but the whole thing isn’t arranged properly, and it’s really hard to tell. The “possible error surface” is large, logical (as opposed to syntactic), and very tricky to unit test. For example, perhaps you forgot to flip your labels when you left-right flipped the image during data augmentation. Your net can still (shockingly) work pretty well because your network can internally learn to detect flipped images and then it left-right flips its predictions. Or maybe your autoregressive model accidentally takes the thing it’s trying to predict as an input due to an off-by-one bug. Or you tried to clip your gradients but instead clipped the loss, causing the outlier examples to be ignored during training. Or you initialized your weights from a pretrained checkpoint but didn’t use the original mean. Or you just screwed up the settings for regularization strengths, learning rate, its decay rate, model size, etc. Therefore, your misconfigured neural net will throw exceptions only if you’re lucky; Most of the time it will train but silently work a bit worse'], retrieval_context=None, additional_metadata={'evolutions': ['Reasoning'], 'synthetic_input_quality': 1.0, 'context_quality': 0.775}, comments=None, tools_called=None, expected_tools=None, source_file='Neural Network Training Recipe.txt'),\n",
       " Golden(input='Explore scenarios where subtle data preprocessing errors significantly degrade neural network performance without raising explicit errors.', actual_output=None, expected_output='When training neural networks, subtle errors, such as forgetting to flip labels during data augmentation or accidentally using the target as input in an autoregressive model, can significantly degrade performance without causing explicit errors. The network may still function, but less effectively, and these logical errors are difficult to detect with unit tests.', context=['the-shelf” technology the second you deviate slightly from training an ImageNet classifier. I’ve tried to make this point in my post “Yes you should understand backprop” by picking on backpropagation and calling it a “leaky abstraction”, but the situation is unfortunately much more dire. Backprop + SGD does not magically make your network work. Batch norm does not magically make it converge faster. RNNs don’t magically let you “plug in” text. And just because you can formulate your problem as RL doesn’t mean you should. If you insist on using the technology without understanding how it works you are likely to fail. Which brings me to…\\n\\n2) Neural net training fails silently\\n\\nWhen you break or misconfigure code you will often get some kind of an exception. You plugged in an integer where something expected a string. The function only expected 3 arguments. This import failed. That key does not exist. The number of elements in the two lists isn’t equal. In addition, it’s often possible to create unit tests for a certain functionality.\\n\\nThis is just a start', ' when it comes to training neural nets. Everything could be correct syntactically, but the whole thing isn’t arranged properly, and it’s really hard to tell. The “possible error surface” is large, logical (as opposed to syntactic), and very tricky to unit test. For example, perhaps you forgot to flip your labels when you left-right flipped the image during data augmentation. Your net can still (shockingly) work pretty well because your network can internally learn to detect flipped images and then it left-right flips its predictions. Or maybe your autoregressive model accidentally takes the thing it’s trying to predict as an input due to an off-by-one bug. Or you tried to clip your gradients but instead clipped the loss, causing the outlier examples to be ignored during training. Or you initialized your weights from a pretrained checkpoint but didn’t use the original mean. Or you just screwed up the settings for regularization strengths, learning rate, its decay rate, model size, etc. Therefore, your misconfigured neural net will throw exceptions only if you’re lucky; Most of the time it will train but silently work a bit worse'], retrieval_context=None, additional_metadata={'evolutions': ['In-Breadth'], 'synthetic_input_quality': 0.8, 'context_quality': 0.775}, comments=None, tools_called=None, expected_tools=None, source_file='Neural Network Training Recipe.txt'),\n",
       " Golden(input='What two qualities strongly correlate to success in deep learning, mitigating suffering during neural network training?', actual_output=None, expected_output='Patience and attention to detail are the two qualities that most strongly correlate to success in deep learning, mitigating suffering during neural network training.', context=['.\\n\\nAs a result, (and this is reeaally difficult to over-emphasize) a “fast and furious” approach to training neural networks does not work and only leads to suffering. Now, suffering is a perfectly natural part of getting a neural network to work well, but it can be mitigated by being thorough, defensive, paranoid, and obsessed with visualizations of basically every possible thing. The qualities that in my experience correlate most strongly to success in deep learning are patience and attention to detail.\\n\\nThe recipe\\n\\nIn light of the above two facts, I have developed a specific process for myself that I follow when applying a neural net to a new problem, which I will try to describe. You will see that it takes the two principles above very seriously. In particular, it builds from simple to complex and at every step of the way we make concrete hypotheses about what will happen and then either validate them with an experiment or investigate until we find some issue. What we try to prevent very hard is the introduction of a lot of “unverified” complexity at once, which is bound to introduce bugs/misconfigurations that will take forever to find (if ever). If writing your neural', 'Andrej Karpathy blog\\nA Recipe for Training Neural Networks\\n\\nAbout\\nAuthor: Andrej Karpathy\\nDate: Apr 25, 2019\\n\\nSome few weeks ago I posted a tweet on “the most common neural net mistakes”, listing a few common gotchas related to training neural nets. The tweet got quite a bit more engagement than I anticipated (including a webinar :)). Clearly, a lot of people have personally encountered the large gap between “here is how a convolutional layer works” and “our convnet achieves state of the art results”.\\n\\nSo I thought it could be fun to brush off my dusty blog to expand my tweet to the long form that this topic deserves. However, instead of going into an enumeration of more common errors or fleshing them out, I wanted to dig a bit deeper and talk about how one can avoid making these errors altogether (or fix them very fast). The trick to doing so is to follow a certain process, which as far as I can tell is not very often documented. Let’s start with two important observations that motivate it.\\n\\n1) Neural net training is a leaky abstraction\\n\\n', ' net code was like training one, you’d want to use a very small learning rate and guess and then evaluate the full test set after every iteration.\\n\\n1. Become one with the data\\n\\nThe first step to training a neural net is to not touch any neural net code at all and instead begin by thoroughly inspecting your data. This step is critical. I like to spend copious amount of time (measured in units of hours) scanning through thousands of examples, understanding their distribution and looking for patterns. Luckily, your brain is pretty good at this. One time I discovered that the data contained duplicate examples. Another time I found corrupted images / labels. I look for data imbalances and biases. I will typically also pay attention to my own process for classifying the data, which hints at the kinds of architectures we’ll eventually explore. As an example - are very local features enough or do we need global context? How much variation is there and what form does it take? What variation is spurious and could be preprocessed out? Does spatial position matter or do we want to average pool it out? How much does detail matter and how far could we afford to downsample the images? How noisy are'], retrieval_context=None, additional_metadata={'evolutions': ['Constrained'], 'synthetic_input_quality': 1.0, 'context_quality': 0.6499999999999999}, comments=None, tools_called=None, expected_tools=None, source_file='Neural Network Training Recipe.txt'),\n",
       " Golden(input='Per Karpathy, before net coding, what initial data-focused step is crucial for effectively training neural networks?', actual_output=None, expected_output='Per Karpathy, the first step to training a neural net is to not touch any neural net code at all and instead begin by thoroughly inspecting your data. This step is critical.', context=['.\\n\\nAs a result, (and this is reeaally difficult to over-emphasize) a “fast and furious” approach to training neural networks does not work and only leads to suffering. Now, suffering is a perfectly natural part of getting a neural network to work well, but it can be mitigated by being thorough, defensive, paranoid, and obsessed with visualizations of basically every possible thing. The qualities that in my experience correlate most strongly to success in deep learning are patience and attention to detail.\\n\\nThe recipe\\n\\nIn light of the above two facts, I have developed a specific process for myself that I follow when applying a neural net to a new problem, which I will try to describe. You will see that it takes the two principles above very seriously. In particular, it builds from simple to complex and at every step of the way we make concrete hypotheses about what will happen and then either validate them with an experiment or investigate until we find some issue. What we try to prevent very hard is the introduction of a lot of “unverified” complexity at once, which is bound to introduce bugs/misconfigurations that will take forever to find (if ever). If writing your neural', 'Andrej Karpathy blog\\nA Recipe for Training Neural Networks\\n\\nAbout\\nAuthor: Andrej Karpathy\\nDate: Apr 25, 2019\\n\\nSome few weeks ago I posted a tweet on “the most common neural net mistakes”, listing a few common gotchas related to training neural nets. The tweet got quite a bit more engagement than I anticipated (including a webinar :)). Clearly, a lot of people have personally encountered the large gap between “here is how a convolutional layer works” and “our convnet achieves state of the art results”.\\n\\nSo I thought it could be fun to brush off my dusty blog to expand my tweet to the long form that this topic deserves. However, instead of going into an enumeration of more common errors or fleshing them out, I wanted to dig a bit deeper and talk about how one can avoid making these errors altogether (or fix them very fast). The trick to doing so is to follow a certain process, which as far as I can tell is not very often documented. Let’s start with two important observations that motivate it.\\n\\n1) Neural net training is a leaky abstraction\\n\\n', ' net code was like training one, you’d want to use a very small learning rate and guess and then evaluate the full test set after every iteration.\\n\\n1. Become one with the data\\n\\nThe first step to training a neural net is to not touch any neural net code at all and instead begin by thoroughly inspecting your data. This step is critical. I like to spend copious amount of time (measured in units of hours) scanning through thousands of examples, understanding their distribution and looking for patterns. Luckily, your brain is pretty good at this. One time I discovered that the data contained duplicate examples. Another time I found corrupted images / labels. I look for data imbalances and biases. I will typically also pay attention to my own process for classifying the data, which hints at the kinds of architectures we’ll eventually explore. As an example - are very local features enough or do we need global context? How much variation is there and what form does it take? What variation is spurious and could be preprocessed out? Does spatial position matter or do we want to average pool it out? How much does detail matter and how far could we afford to downsample the images? How noisy are'], retrieval_context=None, additional_metadata={'evolutions': ['Multi-context'], 'synthetic_input_quality': 1.0, 'context_quality': 0.6499999999999999}, comments=None, tools_called=None, expected_tools=None, source_file='Neural Network Training Recipe.txt'),\n",
       " Golden(input='Compare inspecting data vs. training nets in neural network development, noting the importance of each.', actual_output=None, expected_output='Inspecting data should be the first step when training a neural net. You should spend hours scanning through thousands of examples, understanding their distribution, and looking for patterns before touching any neural net code.', context=[' the labels?\\n\\nIn addition, since the neural net is effectively a compressed/compiled version of your dataset, you’ll be able to look at your network (mis)predictions and understand where they might be coming from. And if your network is giving you some prediction that doesn’t seem consistent with what you’ve seen in the data, something is off.\\n\\nOnce you get a qualitative sense it is also a good idea to write some simple code to search/filter/sort by whatever you can think of (e.g. type of label, size of annotations, number of annotations, etc.) and visualize their distributions and the outliers along any axis. The outliers especially almost always uncover some bugs in data quality or preprocessing.\\n\\n2. Set up the end-to-end training/evaluation skeleton + get dumb baselines\\n\\nNow that we understand our data can we reach for our super fancy Multi-scale ASPP FPN ResNet and begin training awesome models? For sure no. That is the road to suffering. Our next step is to set up a full training + evaluation skeleton and gain trust in its correctness via a series of experiments. At this stage it', '\\n\\nAt this stage we should have a good understanding of the dataset and we have the full training + evaluation pipeline working. For any given model we can (reproducibly) compute a metric that we trust. We are also armed with our performance for an input-independent baseline, the performance of a few dumb baselines (we better beat these), and we have a rough sense of the performance of a human (we hope to reach this). The stage is now set for iterating on a good model.\\n\\nThe approach I like to take to finding a good model has two stages: first get a model large enough that it can overfit (i.e. focus on training loss) and then regularize it appropriately (give up some training loss to improve the validation loss). The reason I like these two stages is that if we are not able to reach a low error rate with any model at all that may again indicate some issues, bugs, or misconfiguration.\\n\\nA few tips & tricks for this stage:\\n\\npicking the model. To reach a good training loss you’ll want to choose an appropriate architecture for the data. When it comes to choosing this my #1 advice is: Don�', ' net code was like training one, you’d want to use a very small learning rate and guess and then evaluate the full test set after every iteration.\\n\\n1. Become one with the data\\n\\nThe first step to training a neural net is to not touch any neural net code at all and instead begin by thoroughly inspecting your data. This step is critical. I like to spend copious amount of time (measured in units of hours) scanning through thousands of examples, understanding their distribution and looking for patterns. Luckily, your brain is pretty good at this. One time I discovered that the data contained duplicate examples. Another time I found corrupted images / labels. I look for data imbalances and biases. I will typically also pay attention to my own process for classifying the data, which hints at the kinds of architectures we’ll eventually explore. As an example - are very local features enough or do we need global context? How much variation is there and what form does it take? What variation is spurious and could be preprocessed out? Does spatial position matter or do we want to average pool it out? How much does detail matter and how far could we afford to downsample the images? How noisy are'], retrieval_context=None, additional_metadata={'evolutions': ['Comparative'], 'synthetic_input_quality': 0.8, 'context_quality': 0.6}, comments=None, tools_called=None, expected_tools=None, source_file='Neural Network Training Recipe.txt'),\n",
       " Golden(input='Before training, how can inspecting data, finding imbalances, and understanding classification enhance NN training?', actual_output=None, expected_output='By thoroughly inspecting your data and understanding their distribution and looking for patterns, you may find corrupted images/labels, data imbalances and biases, and understand your own process for classifying the data.', context=[' the labels?\\n\\nIn addition, since the neural net is effectively a compressed/compiled version of your dataset, you’ll be able to look at your network (mis)predictions and understand where they might be coming from. And if your network is giving you some prediction that doesn’t seem consistent with what you’ve seen in the data, something is off.\\n\\nOnce you get a qualitative sense it is also a good idea to write some simple code to search/filter/sort by whatever you can think of (e.g. type of label, size of annotations, number of annotations, etc.) and visualize their distributions and the outliers along any axis. The outliers especially almost always uncover some bugs in data quality or preprocessing.\\n\\n2. Set up the end-to-end training/evaluation skeleton + get dumb baselines\\n\\nNow that we understand our data can we reach for our super fancy Multi-scale ASPP FPN ResNet and begin training awesome models? For sure no. That is the road to suffering. Our next step is to set up a full training + evaluation skeleton and gain trust in its correctness via a series of experiments. At this stage it', '\\n\\nAt this stage we should have a good understanding of the dataset and we have the full training + evaluation pipeline working. For any given model we can (reproducibly) compute a metric that we trust. We are also armed with our performance for an input-independent baseline, the performance of a few dumb baselines (we better beat these), and we have a rough sense of the performance of a human (we hope to reach this). The stage is now set for iterating on a good model.\\n\\nThe approach I like to take to finding a good model has two stages: first get a model large enough that it can overfit (i.e. focus on training loss) and then regularize it appropriately (give up some training loss to improve the validation loss). The reason I like these two stages is that if we are not able to reach a low error rate with any model at all that may again indicate some issues, bugs, or misconfiguration.\\n\\nA few tips & tricks for this stage:\\n\\npicking the model. To reach a good training loss you’ll want to choose an appropriate architecture for the data. When it comes to choosing this my #1 advice is: Don�', ' net code was like training one, you’d want to use a very small learning rate and guess and then evaluate the full test set after every iteration.\\n\\n1. Become one with the data\\n\\nThe first step to training a neural net is to not touch any neural net code at all and instead begin by thoroughly inspecting your data. This step is critical. I like to spend copious amount of time (measured in units of hours) scanning through thousands of examples, understanding their distribution and looking for patterns. Luckily, your brain is pretty good at this. One time I discovered that the data contained duplicate examples. Another time I found corrupted images / labels. I look for data imbalances and biases. I will typically also pay attention to my own process for classifying the data, which hints at the kinds of architectures we’ll eventually explore. As an example - are very local features enough or do we need global context? How much variation is there and what form does it take? What variation is spurious and could be preprocessed out? Does spatial position matter or do we want to average pool it out? How much does detail matter and how far could we afford to downsample the images? How noisy are'], retrieval_context=None, additional_metadata={'evolutions': ['Reasoning'], 'synthetic_input_quality': 1.0, 'context_quality': 0.6}, comments=None, tools_called=None, expected_tools=None, source_file='Neural Network Training Recipe.txt')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval.synthesizer.config import ContextConstructionConfig\n",
    "\n",
    "model = GeminiChat(LLM_MODEL)\n",
    "embedder = GeminiEmbedding(EMBEDDING_MODEL)\n",
    "synthesizer = Synthesizer(model=model, async_mode=False)\n",
    "\n",
    "synthesizer.generate_goldens_from_docs(\n",
    "    document_paths=[\"../corpus/Neural Network Training Recipe.txt\"],\n",
    "    include_expected_output=True,  # Generate a reference reponse as well\n",
    "    max_goldens_per_context=2,  # From the same set of generated contexts, how many synthetic queries to generate?\n",
    "    context_construction_config=ContextConstructionConfig(\n",
    "        embedder=embedder,\n",
    "        max_contexts_per_document=5,  # How many random (generation + similarity) contexts to create\n",
    "        context_similarity_threshold=0.5,\n",
    "        chunk_size=250,  # this corresponds to token based chunking (so this will translate to 4x-5x number of characters)\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>actual_output</th>\n",
       "      <th>expected_output</th>\n",
       "      <th>context</th>\n",
       "      <th>retrieval_context</th>\n",
       "      <th>n_chunks_per_context</th>\n",
       "      <th>context_length</th>\n",
       "      <th>evolutions</th>\n",
       "      <th>context_quality</th>\n",
       "      <th>synthetic_input_quality</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What qualities correlate strongly to success i...</td>\n",
       "      <td>None</td>\n",
       "      <td>According to Andrej Karpathy's blog, the quali...</td>\n",
       "      <td>[Andrej Karpathy blog\\nA Recipe for Training N...</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>2247</td>\n",
       "      <td>[Constrained]</td>\n",
       "      <td>0.825</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Neural Network Training Recipe.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do patience &amp; attention to detail relate t...</td>\n",
       "      <td>None</td>\n",
       "      <td>According to Andrej Karpathy, the qualities th...</td>\n",
       "      <td>[Andrej Karpathy blog\\nA Recipe for Training N...</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>2247</td>\n",
       "      <td>[Reasoning]</td>\n",
       "      <td>0.825</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Neural Network Training Recipe.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If a novice were to use those libraries, would...</td>\n",
       "      <td>None</td>\n",
       "      <td>No, because neural nets are not \"off-the-shelf...</td>\n",
       "      <td>[It is allegedly easy to get started with trai...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1053</td>\n",
       "      <td>[Hypothetical]</td>\n",
       "      <td>0.775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Neural Network Training Recipe.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Compare the impression given by neural net lib...</td>\n",
       "      <td>None</td>\n",
       "      <td>Neural net libraries give the false impression...</td>\n",
       "      <td>[It is allegedly easy to get started with trai...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1053</td>\n",
       "      <td>[Comparative]</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.7</td>\n",
       "      <td>Neural Network Training Recipe.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why is silently failing during neural net trai...</td>\n",
       "      <td>None</td>\n",
       "      <td>Neural net training often fails silently becau...</td>\n",
       "      <td>[the-shelf” technology the second you deviate ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>2213</td>\n",
       "      <td>[Reasoning]</td>\n",
       "      <td>0.775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Neural Network Training Recipe.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Explore scenarios where subtle data preprocess...</td>\n",
       "      <td>None</td>\n",
       "      <td>When training neural networks, subtle errors, ...</td>\n",
       "      <td>[the-shelf” technology the second you deviate ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>2213</td>\n",
       "      <td>[In-Breadth]</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Neural Network Training Recipe.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What two qualities strongly correlate to succe...</td>\n",
       "      <td>None</td>\n",
       "      <td>Patience and attention to detail are the two q...</td>\n",
       "      <td>[.\\n\\nAs a result, (and this is reeaally diffi...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>3449</td>\n",
       "      <td>[Constrained]</td>\n",
       "      <td>0.650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Neural Network Training Recipe.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Per Karpathy, before net coding, what initial ...</td>\n",
       "      <td>None</td>\n",
       "      <td>Per Karpathy, the first step to training a neu...</td>\n",
       "      <td>[.\\n\\nAs a result, (and this is reeaally diffi...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>3449</td>\n",
       "      <td>[Multi-context]</td>\n",
       "      <td>0.650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Neural Network Training Recipe.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Compare inspecting data vs. training nets in n...</td>\n",
       "      <td>None</td>\n",
       "      <td>Inspecting data should be the first step when ...</td>\n",
       "      <td>[ the labels?\\n\\nIn addition, since the neural...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>3401</td>\n",
       "      <td>[Comparative]</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Neural Network Training Recipe.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Before training, how can inspecting data, find...</td>\n",
       "      <td>None</td>\n",
       "      <td>By thoroughly inspecting your data and underst...</td>\n",
       "      <td>[ the labels?\\n\\nIn addition, since the neural...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>3401</td>\n",
       "      <td>[Reasoning]</td>\n",
       "      <td>0.600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Neural Network Training Recipe.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input actual_output  \\\n",
       "0  What qualities correlate strongly to success i...          None   \n",
       "1  How do patience & attention to detail relate t...          None   \n",
       "2  If a novice were to use those libraries, would...          None   \n",
       "3  Compare the impression given by neural net lib...          None   \n",
       "4  Why is silently failing during neural net trai...          None   \n",
       "5  Explore scenarios where subtle data preprocess...          None   \n",
       "6  What two qualities strongly correlate to succe...          None   \n",
       "7  Per Karpathy, before net coding, what initial ...          None   \n",
       "8  Compare inspecting data vs. training nets in n...          None   \n",
       "9  Before training, how can inspecting data, find...          None   \n",
       "\n",
       "                                     expected_output  \\\n",
       "0  According to Andrej Karpathy's blog, the quali...   \n",
       "1  According to Andrej Karpathy, the qualities th...   \n",
       "2  No, because neural nets are not \"off-the-shelf...   \n",
       "3  Neural net libraries give the false impression...   \n",
       "4  Neural net training often fails silently becau...   \n",
       "5  When training neural networks, subtle errors, ...   \n",
       "6  Patience and attention to detail are the two q...   \n",
       "7  Per Karpathy, the first step to training a neu...   \n",
       "8  Inspecting data should be the first step when ...   \n",
       "9  By thoroughly inspecting your data and underst...   \n",
       "\n",
       "                                             context retrieval_context  \\\n",
       "0  [Andrej Karpathy blog\\nA Recipe for Training N...              None   \n",
       "1  [Andrej Karpathy blog\\nA Recipe for Training N...              None   \n",
       "2  [It is allegedly easy to get started with trai...              None   \n",
       "3  [It is allegedly easy to get started with trai...              None   \n",
       "4  [the-shelf” technology the second you deviate ...              None   \n",
       "5  [the-shelf” technology the second you deviate ...              None   \n",
       "6  [.\\n\\nAs a result, (and this is reeaally diffi...              None   \n",
       "7  [.\\n\\nAs a result, (and this is reeaally diffi...              None   \n",
       "8  [ the labels?\\n\\nIn addition, since the neural...              None   \n",
       "9  [ the labels?\\n\\nIn addition, since the neural...              None   \n",
       "\n",
       "   n_chunks_per_context  context_length       evolutions  context_quality  \\\n",
       "0                     2            2247    [Constrained]            0.825   \n",
       "1                     2            2247      [Reasoning]            0.825   \n",
       "2                     1            1053   [Hypothetical]            0.775   \n",
       "3                     1            1053    [Comparative]            0.775   \n",
       "4                     2            2213      [Reasoning]            0.775   \n",
       "5                     2            2213     [In-Breadth]            0.775   \n",
       "6                     3            3449    [Constrained]            0.650   \n",
       "7                     3            3449  [Multi-context]            0.650   \n",
       "8                     3            3401    [Comparative]            0.600   \n",
       "9                     3            3401      [Reasoning]            0.600   \n",
       "\n",
       "   synthetic_input_quality                         source_file  \n",
       "0                      1.0  Neural Network Training Recipe.txt  \n",
       "1                      1.0  Neural Network Training Recipe.txt  \n",
       "2                      1.0  Neural Network Training Recipe.txt  \n",
       "3                      0.7  Neural Network Training Recipe.txt  \n",
       "4                      1.0  Neural Network Training Recipe.txt  \n",
       "5                      0.8  Neural Network Training Recipe.txt  \n",
       "6                      1.0  Neural Network Training Recipe.txt  \n",
       "7                      1.0  Neural Network Training Recipe.txt  \n",
       "8                      0.8  Neural Network Training Recipe.txt  \n",
       "9                      1.0  Neural Network Training Recipe.txt  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = synthesizer.to_pandas()\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Length (Character Count): 2247\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Dataset Sample</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mDataset Sample\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ <span style=\"color: #808000; text-decoration-color: #808000\">Query:</span>                                                                                                          │\n",
       "│ What qualities correlate strongly to success in DL, according to Karpathy's blog?                               │\n",
       "│                                                                                                                 │\n",
       "│ <span style=\"color: #808000; text-decoration-color: #808000\">Expected Response:</span>                                                                                              │\n",
       "│ According to Andrej Karpathy's blog, the qualities that correlate most strongly to success in deep learning are │\n",
       "│ patience and attention to detail.                                                                               │\n",
       "│                                                                                                                 │\n",
       "│ <span style=\"color: #808000; text-decoration-color: #808000\">Context:</span>                                                                                                        │\n",
       "│ Andrej Karpathy blog                                                                                            │\n",
       "│ A Recipe for Training Neural Networks                                                                           │\n",
       "│                                                                                                                 │\n",
       "│ About                                                                                                           │\n",
       "│ Author: Andrej Karpathy                                                                                         │\n",
       "│ Date: Apr 25, 2019                                                                                              │\n",
       "│                                                                                                                 │\n",
       "│ Some few weeks ago I posted a tweet on “the most common neural net mistakes”, listing a few common gotchas      │\n",
       "│ related to training neural nets. The tweet got quite a bit more engagement than I anticipated (including a      │\n",
       "│ webinar :)). Clearly, a lot of people have personally encountered the large gap between “here is how a          │\n",
       "│ convolutional layer works” and “our convnet achieves state of the art results”.                                 │\n",
       "│                                                                                                                 │\n",
       "│ So I thought it could be fun to brush off my dusty blog to expand my tweet to the long form that this topic     │\n",
       "│ deserves. However, instead of going into an enumeration of more common errors or fleshing them out, I wanted to │\n",
       "│ dig a bit deeper and talk about how one can avoid making these errors altogether (or fix them very fast). The   │\n",
       "│ trick to doing so is to follow a certain process, which as far as I can tell is not very often documented.      │\n",
       "│ Let’s start with two important observations that motivate it.                                                   │\n",
       "│                                                                                                                 │\n",
       "│ 1) Neural net training is a leaky abstraction                                                                   │\n",
       "│                                                                                                                 │\n",
       "│                                                                                                                 │\n",
       "│                                                                                                                 │\n",
       "│ ---                                                                                                             │\n",
       "│                                                                                                                 │\n",
       "│ .                                                                                                               │\n",
       "│                                                                                                                 │\n",
       "│ As a result, (and this is reeaally difficult to over-emphasize) a “fast and furious” approach to training       │\n",
       "│ neural networks does not work and only leads to suffering. Now, suffering is a perfectly natural part of        │\n",
       "│ getting a neural network to work well, but it can be mitigated by being thorough, defensive, paranoid, and      │\n",
       "│ obsessed with visualizations of basically every possible thing. The qualities that in my experience correlate   │\n",
       "│ most strongly to success in deep learning are patience and attention to detail.                                 │\n",
       "│                                                                                                                 │\n",
       "│ The recipe                                                                                                      │\n",
       "│                                                                                                                 │\n",
       "│ In light of the above two facts, I have developed a specific process for myself that I follow when applying a   │\n",
       "│ neural net to a new problem, which I will try to describe. You will see that it takes the two principles above  │\n",
       "│ very seriously. In particular, it builds from simple to complex and at every step of the way we make concrete   │\n",
       "│ hypotheses about what will happen and then either validate them with an experiment or investigate until we find │\n",
       "│ some issue. What we try to prevent very hard is the introduction of a lot of “unverified” complexity at once,   │\n",
       "│ which is bound to introduce bugs/misconfigurations that will take forever to find (if ever). If writing your    │\n",
       "│ neural                                                                                                          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ \u001b[33mQuery:\u001b[0m                                                                                                          │\n",
       "│ What qualities correlate strongly to success in DL, according to Karpathy's blog?                               │\n",
       "│                                                                                                                 │\n",
       "│ \u001b[33mExpected Response:\u001b[0m                                                                                              │\n",
       "│ According to Andrej Karpathy's blog, the qualities that correlate most strongly to success in deep learning are │\n",
       "│ patience and attention to detail.                                                                               │\n",
       "│                                                                                                                 │\n",
       "│ \u001b[33mContext:\u001b[0m                                                                                                        │\n",
       "│ Andrej Karpathy blog                                                                                            │\n",
       "│ A Recipe for Training Neural Networks                                                                           │\n",
       "│                                                                                                                 │\n",
       "│ About                                                                                                           │\n",
       "│ Author: Andrej Karpathy                                                                                         │\n",
       "│ Date: Apr 25, 2019                                                                                              │\n",
       "│                                                                                                                 │\n",
       "│ Some few weeks ago I posted a tweet on “the most common neural net mistakes”, listing a few common gotchas      │\n",
       "│ related to training neural nets. The tweet got quite a bit more engagement than I anticipated (including a      │\n",
       "│ webinar :)). Clearly, a lot of people have personally encountered the large gap between “here is how a          │\n",
       "│ convolutional layer works” and “our convnet achieves state of the art results”.                                 │\n",
       "│                                                                                                                 │\n",
       "│ So I thought it could be fun to brush off my dusty blog to expand my tweet to the long form that this topic     │\n",
       "│ deserves. However, instead of going into an enumeration of more common errors or fleshing them out, I wanted to │\n",
       "│ dig a bit deeper and talk about how one can avoid making these errors altogether (or fix them very fast). The   │\n",
       "│ trick to doing so is to follow a certain process, which as far as I can tell is not very often documented.      │\n",
       "│ Let’s start with two important observations that motivate it.                                                   │\n",
       "│                                                                                                                 │\n",
       "│ 1) Neural net training is a leaky abstraction                                                                   │\n",
       "│                                                                                                                 │\n",
       "│                                                                                                                 │\n",
       "│                                                                                                                 │\n",
       "│ ---                                                                                                             │\n",
       "│                                                                                                                 │\n",
       "│ .                                                                                                               │\n",
       "│                                                                                                                 │\n",
       "│ As a result, (and this is reeaally difficult to over-emphasize) a “fast and furious” approach to training       │\n",
       "│ neural networks does not work and only leads to suffering. Now, suffering is a perfectly natural part of        │\n",
       "│ getting a neural network to work well, but it can be mitigated by being thorough, defensive, paranoid, and      │\n",
       "│ obsessed with visualizations of basically every possible thing. The qualities that in my experience correlate   │\n",
       "│ most strongly to success in deep learning are patience and attention to detail.                                 │\n",
       "│                                                                                                                 │\n",
       "│ The recipe                                                                                                      │\n",
       "│                                                                                                                 │\n",
       "│ In light of the above two facts, I have developed a specific process for myself that I follow when applying a   │\n",
       "│ neural net to a new problem, which I will try to describe. You will see that it takes the two principles above  │\n",
       "│ very seriously. In particular, it builds from simple to complex and at every step of the way we make concrete   │\n",
       "│ hypotheses about what will happen and then either validate them with an experiment or investigate until we find │\n",
       "│ some issue. What we try to prevent very hard is the introduction of a lot of “unverified” complexity at once,   │\n",
       "│ which is bound to introduce bugs/misconfigurations that will take forever to find (if ever). If writing your    │\n",
       "│ neural                                                                                                          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.panel import Panel\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "row_index = 0\n",
    "first_row = dataframe.iloc[row_index].to_dict()\n",
    "\n",
    "print(\"Context Length (Character Count):\", len(\"\".join(first_row[\"context\"])))\n",
    "console.print(\"\\n[bold green]Dataset Sample[/bold green]\")\n",
    "console.print(\n",
    "    Panel.fit(\n",
    "        f\"[yellow]Query:[/yellow]\\n{first_row['input']}\\n\\n\"\n",
    "        f\"[yellow]Expected Response:[/yellow]\\n{first_row['expected_output']}\\n\\n\"\n",
    "        f\"[yellow]Context:[/yellow]\\n\"\n",
    "        + \"\\n\\n---\\n\\n\".join(\n",
    "            [context.replace(\"\\\\n\", \"\\n\") for context in first_row[\"context\"]]\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ask-notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
